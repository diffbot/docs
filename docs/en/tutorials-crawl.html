<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Crawlbot walkthrough · Docs Suite</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="Crawlbot allows you to apply a Diffbot API to entire sites, returning the complete structured data in JSON or CSV format."/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Crawlbot walkthrough · Docs Suite"/><meta property="og:type" content="website"/><meta property="og:url" content="https://diffbot.github.io/"/><meta property="og:description" content="Crawlbot allows you to apply a Diffbot API to entire sites, returning the complete structured data in JSON or CSV format."/><meta property="og:image" content="https://diffbot.github.io/img/diagram.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://diffbot.github.io/img/diagram.svg"/><link rel="shortcut icon" href="/img/diffbot-head.png"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.css"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="/js/app.js"></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/diffbot_white.svg" alt="Docs Suite"/><h2 class="headerTitleWithLogo">Docs Suite</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/docs/en/error-intro" target="_self">Debugging</a></li><li class="navSearchWrapper reactNavSearchWrapper"><input type="text" id="search_input_react" placeholder="Search" title="Search"/></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="container mainContainer docsContainer"><div class="wrapper"><div class="post"><header class="postHeader"><a class="edit-page-link button" href="https://github.com/diffbot/docs/edit/master/docs/tutorials-crawl.md" target="_blank" rel="noreferrer noopener">Edit</a><h1 id="__docusaurus" class="postHeaderTitle">Crawlbot walkthrough</h1></header><article><div><span><p>Crawlbot allows you to apply a Diffbot API to entire sites, returning the complete structured data in JSON or CSV format.</p>
<h3><a class="anchor" aria-hidden="true" id="overview"></a><a href="#overview" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Overview</h3>
<p>Crawlbot works hand-in-hand with a <a href="api-basics-index">Diffbot API</a> (either automatic or custom). It quickly spiders a site for appropriate links and hands these links to a Diffbot API for processing. All structured page results are then compiled into a single &quot;collection,&quot; which can be downloaded in full or searched using the <a href="cb-basics-search">Search API</a>.</p>
<p>Crawlbot is limited to Extraction API Plus plans and above, and is accessible in the Developer Dashboard here.</p>
<p>Note that the limit of active crawls on a single token is 1000. More information <a href="error-too-many-collections">here</a>.</p>
<h3><a class="anchor" aria-hidden="true" id="creating-a-crawl-basics"></a><a href="#creating-a-crawl-basics" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Creating a crawl: basics</h3>
<p>Each crawl requires, at minimum, the following:</p>
<ul>
<li>a name (e.g. &quot;DiffbotCrawl&quot;)</li>
<li>a starting or &quot;seed&quot; URL. Multiple URLs can be provided to process more than one site in the same crawl. If the seed contains a non-www subdomain (&quot;<a href="http://blog.diffbot.com">http://blog.diffbot.com</a>&quot; or &quot;<a href="https://docs.diffbot.com">https://docs.diffbot.com</a>&quot;) Crawlbot will restrict spidering to the specified subdomain. If you wish to expand your crawl to multiple domains, enter each one as a separate seed; or consider the <code>restrictDomain</code> setting (<a href="guides-restrict-domain">more info here</a>).</li>
<li>a Diffbot API to be used for processing pages.</li>
</ul>
<p><img src="/img/choose_api.png" alt="Choosing the API"></p>
<h3><a class="anchor" aria-hidden="true" id="creating-an-automatic-crawl-using-the-analyze-api"></a><a href="#creating-an-automatic-crawl-using-the-analyze-api" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Creating an Automatic Crawl Using the Analyze API</h3>
<p>The simplest Crawlbot crawl will apply Diffbot's <a href="api-basics-analyze">Analyze API</a> to a single site. The Analyze API determines the &quot;page-type&quot; of submitted URLs, and automatically extracts the contents if the page-type is currently supported.</p>
<p>To run your crawl:</p>
<ul>
<li>enter a crawl name</li>
<li>enter a seed URL like <a href="https://blog.diffbot.com">https://blog.diffbot.com</a></li>
<li>select the Analyze API from the &quot;Diffbot API&quot; menu</li>
<li>click &quot;Start&quot;</li>
</ul>
<p>Each page found on the site will be analyzed and all supported page-types (article, discussion, image, product, etc.) will be automatically extracted and made available in the resulting collection.</p>
<h3><a class="anchor" aria-hidden="true" id="creating-a-crawl-using-a-specific-extraction-api"></a><a href="#creating-a-crawl-using-a-specific-extraction-api" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Creating a Crawl Using a Specific Extraction API</h3>
<p>If you don't want to rely on the Analyze API's analysis of pages, you can use a specific Diffbot extraction API (e.g., <a href="api-basics-product">Product</a> or <a href="api-basics-article">Article API</a>). Because by default all pages on a domain will be processed by the API you select, in nearly all of these circumstances you will want to further narrow your crawl using the Crawl or Processing Patterns (or regular expressions), discussed below.</p>
<h3><a class="anchor" aria-hidden="true" id="controlling-or-limiting-pages-to-crawl"></a><a href="#controlling-or-limiting-pages-to-crawl" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Controlling or Limiting Pages to Crawl</h3>
<blockquote>
<p>Note: Know the difference between <a href="explain-crawling-versus-processing">crawling and processing</a></p>
</blockquote>
<p>By default, Crawlbot will spider and process all links on the domain(s) matching your seed URL(s). You can limit which pages of a site to crawl (spider for links), and which pages of a site to process, using the &quot;Crawling Limits&quot; and &quot;Page Processing Limits&quot; sections.</p>
<h4><a class="anchor" aria-hidden="true" id="crawl-patterns"></a><a href="#crawl-patterns" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Crawl Patterns</h4>
<p>Crawl patterns will limit crawling to only those URLs containing at least one of the matching strings. You may enter as many crawl patterns as you like, one per line. Any URLs not containing a match will be ignored.</p>
<p>For example, to limit crawling at <code>diffbot.com</code> to only &quot;blog.diffbot.com&quot; pages, you can enter a crawl pattern of <code>blog.diffbot.com</code>. If you only wanted to crawl the &quot;Shoes&quot; section of a site, you might enter a crawl pattern of <code>/category/shoes</code>.</p>
<p>You can also supply <strong>negative</strong> crawl patterns by prepending your pattern with a &quot;!&quot; (exclamation point). You can supply as many negative patterns as you like. All matching URLs will be skipped.</p>
<p><img src="/img/crawl_patterns.png" alt="Restricting the spider to &quot;category&quot; URLs and preventing spidering of author and page URLs"></p>
<p>This restricts the spider to &quot;category&quot; URLs, and prevents any URLs with &quot;/author/&quot; or &quot;/page/&quot; from being spidered.</p>
<p>You can also use the <code>^</code> and <code>$</code> characters - borrowed from regular-expression syntax - to specify the start and end of a pattern. For instance, <code>^http://www.diffbot.com</code> will match URLs starting with &quot;<a href="http://www.diffbot.com">http://www.diffbot.com</a>,&quot; and <code>type=product$</code> will match all URLs ending in &quot;type=product.&quot;</p>
<h4><a class="anchor" aria-hidden="true" id="page-progcessing-patterns"></a><a href="#page-progcessing-patterns" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Page Progcessing Patterns</h4>
<p>Page processing patterns are identical to crawling patterns, except they define which pages will be <strong>processed</strong> by the selected Diffbot API.</p>
<blockquote>
<p>Note:If you limit site crawling via crawl patterns or regular expressions, but do not enter processing patterns, <strong>only pages matching your crawl patterns/regexes</strong> will be processed. A crawling restriction will serve for both crawling and processing in the absence of a dedicated processing restriction.</p>
</blockquote>
<p>For example, to only process product pages for a site you might enter a page processing pattern of &quot;/product/detail/&quot; - this would match a URL like <a href="http://shopping.diffbot.com/product/detail/8117a7?name=diffy_robot">http://shopping.diffbot.com/product/detail/8117a7?name=diffy_robot</a>, but would not match the URLs <a href="http://shopping.diffbot.com">http://shopping.diffbot.com</a> or <a href="http://shopping.diffbot.com/category/plush">http://shopping.diffbot.com/category/plush</a>.</p>
<p><img src="/img/processing_patterns.png" alt="This restricts processing to URLs containing &quot;/product/detail/&quot; -- and will skip any such URLs also containing  &quot;?currency=euro.&quot;">
This restricts processing to URLs containing &quot;/product/detail/&quot; -- and will skip any such URLs also containing  &quot;?currency=euro.&quot;</p>
<h4><a class="anchor" aria-hidden="true" id="crawl-and-page-processing-regular-expressions"></a><a href="#crawl-and-page-processing-regular-expressions" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Crawl and Page Processing Regular Expressions</h4>
<p>If you know your way around regular expressions, you can write a crawl or page processing regex in place of patterns.
If you supply a regex, any patterns will be ignored: only the URLs that contain a match to your provided expression will be crawled and/or processed.</p>
<h4><a class="anchor" aria-hidden="true" id="html-processing-patterns"></a><a href="#html-processing-patterns" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>HTML Processing Patterns</h4>
<p>The HTML Processing Pattern allows you to require exact <strong>markup</strong> strings that must be found in the page HTML in order for a page to be processed. For example, if a site's articles all contain the string:</p>
<pre><code class="hljs css language-html"><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">"articleBody"</span>&gt;</span>
</code></pre>
<p>You can specify an HTML Processing Pattern of <code>class=&quot;articleBody&quot;</code> to limit processing only to the article pages (those containing the string in their HTML).</p>
<h4><a class="anchor" aria-hidden="true" id="pages-to-process--pages-to-crawl"></a><a href="#pages-to-process--pages-to-crawl" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Pages to Process / Pages to Crawl</h4>
<p>You can also set a fixed number of pages to crawl or process. Your crawl will complete as soon as one of these is reached.</p>
<p>By default, crawls are set to process and crawl a maximum of 100,000 pages.</p>
<h3><a class="anchor" aria-hidden="true" id="passing-diffbot-api-querystring-arguments"></a><a href="#passing-diffbot-api-querystring-arguments" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Passing Diffbot API Querystring Arguments</h3>
<p>Crawlbot hands off URLs to <a href="https://diffbot.com/products/automatic">specific Diffbot APIs</a> for processing. Each of these APIs has optional querystring arguments that can be used to modify the information returned -- most commonly the <code>fields</code> argument, for adding optional fields to the Diffbot response.</p>
<h3><a class="anchor" aria-hidden="true" id="robotstxt-and-politeness"></a><a href="#robotstxt-and-politeness" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Robots.txt and Politeness</h3>
<p>By default Crawlbot adheres to <a href="http://en.wikipedia.org/wiki/Robots_exclusion_standard">robots.txt instructions</a> instructions, including the crawlDelay parameter. In rare cases -- e.g., when crawling a partner's site with permission -- Crawlbot can be configured to ignore <code>robots.txt</code> instructions.</p>
<p>Crawlbot also has a default &quot;politeness&quot; setting of 0.25 seconds -- spidering machines will wait a quarter of a second between page-loads in order to minimize the impact of sites being crawled. You can adjust this on a per-crawl basis.</p>
<h3><a class="anchor" aria-hidden="true" id="repeating-crawls"></a><a href="#repeating-crawls" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Repeating Crawls</h3>
<p>You can optionally set crawl jobs to repeat automatically. Each repeat crawl &quot;round&quot; will fully re-spider from each seed URL, and process pages according to your repeat settings.</p>
<p>For each URL:</p>
<ul>
<li>If the URL was previously processed, and the extracted data is different (e.g., a price change), the data will be updated and the <code>timestamp</code> value in the JSON response will be updated to indicate an updated record.</li>
<li>If the URL was previously processed and the extracted data is the same as the previous round (no changes), the data will remain the same and the <code>timestamp</code> value in the JSON response will not be updated.</li>
<li>If the URL is brand new, it will be processed for the first time.</li>
</ul>
<p>You can optionally choose to &quot;only process if new,&quot; which will only process newly-discovered URLs. In this case, pages of a site will only be processed once -- regardless of the number of rounds. If &quot;only process if new&quot; is not selected, all pages in the site (and matching the process pattern / regular expression) will be processed each round.</p>
<p>You will also need to indicate:</p>
<ul>
<li>Repeat frequency: this is indicated in days (&quot;1&quot; for daily; &quot;7&quot; for weekly; &quot;0.25&quot; for every six hours).</li>
<li>Number of repeats. Set to 0 or leave blank to repeat indefinitely.</li>
</ul>
<p><img src="/img/repeat_crawl.png" alt="This recurring crawl will repeat every 12 hours, only picking up new pages, for two weeks."></p>
<p>Repeat timing is based on the <strong>end</strong> of the previous crawl round. So if a daily (&quot;1&quot;) repeating crawl ends on Tuesday at 12:00pm, the next round will <strong>start</strong> on Wednesday at noon. Using the Crawlbot API you can use the <strong>roundStart</strong> argument to control when your repeating crawl rounds start.</p>
<h3><a class="anchor" aria-hidden="true" id="notifications"></a><a href="#notifications" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Notifications</h3>
<p>You can choose to be notified at the conclusion of each bulk job, either by <a href="http://en.wikipedia.org/wiki/Webhook">webhook</a> or email.</p>
<p>For webhook, you will need to supply a URL that is capable of receiving a POST request. One alternative to building your own: use the Diffbot app on <a href="https://zapier.com/zapbook/diffbot/">Zapier</a> to receive webhook notifications.</p>
<h3><a class="anchor" aria-hidden="true" id="accessing-crawl-data"></a><a href="#accessing-crawl-data" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Accessing Crawl Data</h3>
<p>You can access processed data anytime during a crawl, or after it completes. Crawlbot offers two download options within the interface:</p>
<ul>
<li>Full JSON Output: A single file, in valid JSON, containing all of the processed objects from your crawl.</li>
<li>CSV Output: A single comma-separated-values file of the <strong>top-level</strong> objects. Nested elements (article images, tags, etc.) will not be returned in the CSV.</li>
</ul>
<p>If you only want to access a subset of your data, the <a href="cb-basics-search">Search API</a> allows much more flexibility in searching and retrieving only the matching items from queries.</p>
<h3><a class="anchor" aria-hidden="true" id="the-url-report"></a><a href="#the-url-report" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The URL Report</h3>
<p>Also provided with each crawl is a downloadable (CSV) report on each page crawled and/or processed within a crawling job. This file is a log of each URL analyzed by Crawlbot, and is useful in diagnosing issues -- &quot;Why wasn't this page processed?&quot; -- or simply cataloguing which URLs have been processed.</p>
<p>Each row in the URLs report contains diagnostic information, including:</p>
<p>-Whether the URL was processed by a Diffbot extraction API
-Crawling timestamp
-Last status of the URL</p>
<p><a href="explain-crawl-url-report">See complete details on the URL Report</a>.</p>
</span></div></article></div><div class="docLastUpdate"><em>Last updated by Bruno Skvorc</em></div><div class="docs-prevnext"></div></div></div><nav class="onPageNav"></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><a href="/" class="nav-home"><img src="/img/diffbot_white.svg" alt="Docs Suite" width="66" height="58"/></a><div><h5>Docs</h5><a href="/docs/en/api-basics-index">Extraction</a><a href="/docs/en/cb-basics-index">Crawling</a><a href="/docs/en/kg-index">Knowledge Graph</a><a href="/docs/en/explain-gdpr">Diffbot and GDPR</a></div><div><h5>Community</h5><a href="https://stackoverflow.com/questions/tagged/diffbot" target="_blank" rel="noreferrer noopener">Stack Overflow</a><a href="https://twitter.com/diffbot" target="_blank" rel="noreferrer noopener">Twitter</a></div><div><h5>More</h5><a href="/blog">Blog</a><a href="/help">Help</a><a href="https://github.com/diffbot">GitHub</a></div></section><a href="https://diffbot.com/" target="_blank" rel="noreferrer noopener" class="fbOpenSource"><img src="/img/diffbot_white.svg" alt="Diffbot.com" width="170" height="45"/></a><section class="copyright">Copyright © 2024 Diffbot.com</section></footer></div><script type="text/javascript" src="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.js"></script><script>
                document.addEventListener('keyup', function(e) {
                  if (e.target !== document.body) {
                    return;
                  }
                  // keyCode for '/' (slash)
                  if (e.keyCode === 191) {
                    const search = document.getElementById('search_input_react');
                    search && search.focus();
                  }
                });
              </script><script>
              var search = docsearch({
                
                apiKey: '2c24e5e78d724e77c46ef1b720700177',
                indexName: 'diffbot',
                inputSelector: '#search_input_react',
                algoliaOptions: {}
              });
            </script></body></html>
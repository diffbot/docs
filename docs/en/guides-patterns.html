<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Crawl and Processing Patterns and Regexes · Docs Suite</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="Crawlbot offers many ways to manually narrow or refine the pages crawled or processed by Diffbot APIs."/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Crawl and Processing Patterns and Regexes · Docs Suite"/><meta property="og:type" content="website"/><meta property="og:url" content="https://diffbot.github.io/"/><meta property="og:description" content="Crawlbot offers many ways to manually narrow or refine the pages crawled or processed by Diffbot APIs."/><meta property="og:image" content="https://diffbot.github.io/img/diagram.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://diffbot.github.io/img/diagram.svg"/><link rel="shortcut icon" href="/img/diffbot-head.png"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.css"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="/js/app.js"></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/diffbot_white.svg" alt="Docs Suite"/><h2 class="headerTitleWithLogo">Docs Suite</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/docs/en/error-intro" target="_self">Debugging</a></li><li class="navSearchWrapper reactNavSearchWrapper"><input type="text" id="search_input_react" placeholder="Search" title="Search"/></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="container mainContainer docsContainer"><div class="wrapper"><div class="post"><header class="postHeader"><a class="edit-page-link button" href="https://github.com/diffbot/docs/edit/master/docs/guides-patterns.md" target="_blank" rel="noreferrer noopener">Edit</a><h1 id="__docusaurus" class="postHeaderTitle">Crawl and Processing Patterns and Regexes</h1></header><article><div><span><p>Crawlbot offers many ways to manually narrow or refine the pages crawled or processed by Diffbot APIs.</p>
<p>(<a href="explain-crawling-versus-processing">Read our overview of crawling vs processing</a>)</p>
<h2><a class="anchor" aria-hidden="true" id="patterns-crawl-and-processing"></a><a href="#patterns-crawl-and-processing" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Patterns (&quot;Crawl&quot; and &quot;Processing&quot;)</h2>
<p>Patterns allow you to quickly and easily restrict pages crawled or processed based on simple URL string matches.</p>
<p>For example, if a web site organizes its pages under categories — e.g., <a href="http://www.example.com/sports/heres-a-sports-article.html">http://www.example.com/sports/heres-a-sports-article.html</a> — I can instruct Crawlbot to only crawl pages within the &quot;sports&quot; category by specifying a crawl pattern of <code>/sports/</code>. (Including the slashes is even more precise and makes sure not to match a &quot;sports&quot; string elsewhere in the URL.)</p>
<p>I can also use a crawl pattern if I want to limit crawling to a particular subdomain. For instance, on a crawl starting at <a href="https://docs.diffbot.com">https://docs.diffbot.com</a>, I can enter a crawl pattern of <code>docs.diffbot.com</code> to keep Crawlbot from following links to <a href="http://www.diffbot.com">http://www.diffbot.com</a> and <a href="http://blog.diffbot.com">http://blog.diffbot.com</a>.</p>
<p>You can enter multiple patterns to match multiple strings. For instance, to crawl both <a href="https://docs.diffbot.com">https://docs.diffbot.com</a> and <a href="http://blog.diffbot.com">http://blog.diffbot.com</a> (but not <a href="http://www.diffbot.com">http://www.diffbot.com</a>), I would enter a crawl pattern of:</p>
<pre><code class="hljs"><span class="hljs-selector-tag">docs</span><span class="hljs-selector-class">.diffbot</span><span class="hljs-selector-class">.com</span>
<span class="hljs-selector-tag">blog</span><span class="hljs-selector-class">.diffbot</span><span class="hljs-selector-class">.com</span>
</code></pre>
<p>In the Crawlbot interface, place each individual pattern on a new line. Via the API, separate patterns with a <code>||</code>.</p>
<h2><a class="anchor" aria-hidden="true" id="limiting-matches-to-the-beginning-of-urls"></a><a href="#limiting-matches-to-the-beginning-of-urls" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Limiting Matches to the Beginning of URLs</h2>
<p>You can use the caret character (<code>^</code>) to limit pattern matches only to the beginning of a URL. For instance, a processing pattern of:</p>
<pre><code class="hljs"><span class="hljs-symbol">^https</span>:<span class="hljs-comment">//docs.diffbot.com</span>
</code></pre>
<p>...will limit processing only to pages whose URLs begin with <a href="https://docs.diffbot.com">https://docs.diffbot.com</a>. This will prevent processing of URLs like <a href="http://www.twitter.com/share?tweet=https://docs.diffbot.com">http://www.twitter.com/share?tweet=https://docs.diffbot.com</a>.</p>
<h2><a class="anchor" aria-hidden="true" id="negative-match-patterns"></a><a href="#negative-match-patterns" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Negative-Match Patterns</h2>
<p>Use the exclamation-point to specify a &quot;negative match&quot; if you want to explicitly exclude pages from being crawled or processed. For instance, to process all pages except those containing &quot;sports&quot; in the URL, I would enter a crawl pattern of <code>!sports</code>.</p>
<p>When entering multiple patterns, negative matches will override other crawl patterns. That is, a URL with a negative match will be fully ignored, even if another (positive) crawl pattern is also a match.</p>
<h2><a class="anchor" aria-hidden="true" id="regular-expressions-crawl-and-processing-regexes"></a><a href="#regular-expressions-crawl-and-processing-regexes" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Regular Expressions (Crawl and Processing Regexes)</h2>
<p>(<a href="explain-regex">Which regular expression syntax does Crawlbot use?</a>)</p>
<p>If you want complete control over your crawling or processing URL matches, you can write a regular expression to only crawl or process URLs that contain a match to your expression.</p>
<p>For example, to only process pages at <a href="https://docs.diffbot.com/">https://docs.diffbot.com/</a> under the &quot;/crawlbot&quot; path and containing &quot;regex&quot;, you could enter a processing regex of:</p>
<pre><code class="hljs">\<span class="hljs-string">/crawlbot.</span>*?regex
</code></pre>
<p>Note that crawling and processing regular expressions cannot be used simultaneously with crawling/processing patterns. If both are provided, the crawling/processing patterns will be ignored.</p>
<h2><a class="anchor" aria-hidden="true" id="html-processing-patterns"></a><a href="#html-processing-patterns" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>HTML Processing Patterns</h2>
<p>Crawlbot offers one more option for limiting pages processed. If you enter an HTML Processing Pattern, only pages whose HTML source contains the exact string will be processed.</p>
<p>Note that Crawlbot only examines the raw source, and does not execute Javascript/Ajax at crawl-time.</p>
<h2><a class="anchor" aria-hidden="true" id="implications-for-crawl-performance"></a><a href="#implications-for-crawl-performance" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Implications for Crawl Performance</h2>
<p>(<a href="explain-how-long-crawl-site">How long does it take to crawl a site?</a>)</p>
<p>While there are many factors that will influence how long it takes to crawl a given site, one of the best ways to speed up your crawl is to use crawling and processing patterns or regular expressions to limit Crawlbot just to the pages you are interested in. For more on this, see <a href="explain-improving-crawl-efficiency">Improving Crawl Efficiency</a>.</p>
</span></div></article></div><div class="docLastUpdate"><em>Last updated by Dan Urman</em></div><div class="docs-prevnext"></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#patterns-crawl-and-processing">Patterns (&quot;Crawl&quot; and &quot;Processing&quot;)</a></li><li><a href="#limiting-matches-to-the-beginning-of-urls">Limiting Matches to the Beginning of URLs</a></li><li><a href="#negative-match-patterns">Negative-Match Patterns</a></li><li><a href="#regular-expressions-crawl-and-processing-regexes">Regular Expressions (Crawl and Processing Regexes)</a></li><li><a href="#html-processing-patterns">HTML Processing Patterns</a></li><li><a href="#implications-for-crawl-performance">Implications for Crawl Performance</a></li></ul></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><a href="/" class="nav-home"><img src="/img/diffbot_white.svg" alt="Docs Suite" width="66" height="58"/></a><div><h5>Docs</h5><a href="/docs/en/api-basics-index">Extraction</a><a href="/docs/en/cb-basics-index">Crawling</a><a href="/docs/en/kg-index">Knowledge Graph</a><a href="/docs/en/explain-gdpr">Diffbot and GDPR</a></div><div><h5>Community</h5><a href="https://stackoverflow.com/questions/tagged/diffbot" target="_blank" rel="noreferrer noopener">Stack Overflow</a><a href="https://twitter.com/diffbot" target="_blank" rel="noreferrer noopener">Twitter</a></div><div><h5>More</h5><a href="/blog">Blog</a><a href="/help">Help</a><a href="https://github.com/diffbot">GitHub</a></div></section><a href="https://diffbot.com/" target="_blank" rel="noreferrer noopener" class="fbOpenSource"><img src="/img/diffbot_white.svg" alt="Diffbot.com" width="170" height="45"/></a><section class="copyright">Copyright © 2021 Diffbot.com</section></footer></div><script type="text/javascript" src="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.js"></script><script>
                document.addEventListener('keyup', function(e) {
                  if (e.target !== document.body) {
                    return;
                  }
                  // keyCode for '/' (slash)
                  if (e.keyCode === 191) {
                    const search = document.getElementById('search_input_react');
                    search && search.focus();
                  }
                });
              </script><script>
              var search = docsearch({
                
                apiKey: '2c24e5e78d724e77c46ef1b720700177',
                indexName: 'diffbot',
                inputSelector: '#search_input_react',
                algoliaOptions: {}
              });
            </script></body></html>